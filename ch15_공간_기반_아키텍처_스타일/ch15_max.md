# 15장. 공간 기반 아키텍처 스타일

- 웹 기반 애플리케이션 요청 흐름
    - 브라우저 요청 → 웹 서버 → 애플리케이션 서버 → 데이터베이스 서버
    - 유저가 늘어나면 병목현상
    - 처음에는 웹 서버 레이어에서 → 나중에는 애플리케이션, 데이터베이스 서버 레이어에서도 발생

**삼각형 토폴로지**
<img width="703" alt="스크린샷 2024-11-17 오후 7 53 37" src="https://github.com/user-attachments/assets/119c194c-4b0c-4a2b-9dcc-5668b73ac704">

- 웹 서버
    - 가장 길고 확장하기 쉬운 변이. 초기에는 웹 서버에 병목이 발생
    - 비교적 저렴하고 손쉽게 확장 가능
- 애플리케이션 서버
    - 중간 길이의 변이, 확장이 더 복잡하고 비용이 많이 든다.
    - 웹 서버가 충분히 확장되면, 병목 현상이 애플리케이션 서버로 이동
- 데이터베이스 서버
    - 가장 짧고 확장하기 어려운 변이.
    - 최후의 병목 지점
    - 유저 부하가 계속 증가하면 결국 데이터베이스 서버에서 병목이 발생
    - 확장은 매우 어려우며 비용도 많이 든다.

→ 공간 기반 아키텍처 스타일은 높은 확장성, 탄력성, 동시성 및 이와 관련된 문제를 해결하기 위해 설계

- 동시 유저 수가 매우 가변적이라서 예측조차 곤란한 애플리케이션에서도 유용
- 극단적이고 가변적인 확장성 문제는 DB확장 or 캐시 기술을 적용하는 것보다 아키텍처적으로 해결하는 것이 더 낫다.

---

## 15.1 토폴로지

- 명칭은 튜플 공간에서 유래
    - 튜플 공간 : 공유 메모리를 통해 통신하는 다중 병렬 프로세서를 사용하는 기술
- 시스템에서 동기 제약 조건인 중앙 데이터베이스를 없애는 대신, 복제된 인메모리 데이터 그리드를 활용 → 확장성 탄력성을 높일 수 있다.
- 데이터는 메모리에서 빠르게 처리하고, 데이터베이스는 나중에 업데이트하므로 속도가 빠르다.
- 유저 부하에 따라 서버를 늘리거나 줄일 수 있어 거의 무한한 확장성.

**구성**

- 처리장치
    - 애플리케이션 코드가 실제로 실행되는 곳
- 가상 미들웨어
    - 처리 장치를 관리하고 조정하는 가상화된 미들웨어 계층
- 데이터 펌프
    - 업데이트된 데이터를 비동기적으로 데이터베이스에 전송하는 역할
- 데이터 라이터
    - 데이터 펌프에서 전송된 데이터를 받아 데이터베이스에 최종적으로 영구 저장하는 컴포넌트
- 데이터 리더
    - 새로운 처리 장치가 시작될 때, 데이터베이스의 데이터를 읽어와서 메모리에 초기화하는 역할
    - 
<img width="692" alt="스크린샷 2024-11-17 오후 7 53 53" src="https://github.com/user-attachments/assets/e5dce759-3c3e-4810-8e5a-ec785c785f5c">

---

### 15.1.1 처리 장치

1. 역할
- 애플리케이션 로직을 포함
- 주로 웹 기반 컴포넌트와 백엔드 비즈니스 로직을 담당.

2. 배포 방식
- 소규모 웹 애플리케이션 : 단일 처리 장치에 배포 가능
- 대규모 웹 애플리케이션 : 여러 처리 장치로 기능을 나누어 배포

3. 인메모리 데이터 그리드 통합
- 처리 장치에는 헤이즐캐스트, 아파치 이그나이트, 오라클 코히어런스 등의 제품에 있는 인메모리 데이터 그리드 및 복제 엔진도 포함될 수 있다.
- 
<img width="662" alt="스크린샷 2024-11-17 오후 7 54 23" src="https://github.com/user-attachments/assets/2bc4cafe-ee11-4fbc-978c-9c9af3843429">

---

### 15.1.2 가상 미들웨어

- 공간 기반 아키텍처에서 데이터 동기화와 요청 처리를 관리하는 인프라.

1. **메시징 그리드**

- 역할 : 입력 요청과 세션 상태 관리
- 작동 방식 : 가상 미들웨어에 요청이 들어오면 메시징 그리드가 활성 처리 장치중 하나를 선택하여 요청을 전달.

<img width="348" alt="스크린샷 2024-11-17 오후 7 54 53" src="https://github.com/user-attachments/assets/c1314031-cf1a-46fa-9266-fc66295ccdc4">

2. **데이터 그리드**

- 데이터 그리드와 복제 캐시
    - 데이터 그리드는 여러 처리 장치 간에 동일한 데이터를 공유하고 동기화하는 중요한 컴포넌트
    - 대부분 복제 캐시 형태로 구현되어, 각 처리 장치가 같은 데이터를 유지
- 메시징 그리드
    - 메시징 그리드는 요청을 처리 장치 간에 전달하여 데이터를 동기화
- 비동기 방식의 데이터 동기화
    - 데이터 동기화는 비동기 방식으로 빠르게 이루어지며, 보통 100밀리초 이내에 완료
    
<img width="342" alt="스크린샷 2024-11-17 오후 7 55 10" src="https://github.com/user-attachments/assets/e78eefda-622b-4f17-b100-937a073ae017">


- Hazelcast 인스턴스 생성 및 데이터 그리드 설정
    
    ```jsx
    HazelcastInstance hz = Hazelcast.newHazelcastInstance();
    Map<String, CustomerProfile> profileCache = hz.getReplicatedMap("CustomerProfile");
    ```
    
    - 이 코드는 고객 프로필 정보를 저장하는 복제 캐시(`CustomerProfile`)를 설정.
    - `HazelcastInstance`는 Hazelcast 클러스터에서 처리 장치를 관리하고, `getReplicatedMap`을 사용해 고객 프로필 캐시를 가져온다.
    - 이 캐시는 모든 처리 장치에서 동일하게 유지되며, 각 장치에서 변경 사항이 발생하면 다른 처리 장치로 복제됩니다.
    
- 처리 장치 간 캐시 동기화
    - 복제 캐시(`CustomerProfile`)에 변경이 일어나면
    - 모든 처리 장치에서 해당 캐시가 동일하게 업데이트
    - 예를 들어, 한 처리 장치에서 `cache.put()`을 사용해 데이터를 변경하면, 모든 처리 장치에서 이 변경 사항이 비동기적으로 동기화
    
- 처리 장치 인스턴스와 멤버 리스트
    - 각 처리 장치는 `memberList`를 사용하여 다른 처리 장치를 인식
    - 멤버 리스트는 클러스터 내 다른 처리 장치들의 IP 주소와 포트를 포함
    - 예를 들어, 첫 번째 인스턴스가 실행되면, 멤버 리스트는 자기 자신만 포함
    
    ```jsx
    Instance 1:
    Members {size:1, ver:1} [Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268 this
    ]
    ```
    

- 두 번째 인스턴스 추가 시 멤버 리스트 업데이트

```jsx
Instance 1:
Members {size:2, ver:2} [
    Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268 this
    Member [172.19.248.90]:5702 - ea9e4dd5-5cb3-4b27-8fe8-db5cc62c7316
]

Instance 2:
Members {size:2, ver:2} [
    Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268
    Member [172.19.248.90]:5702 - ea9e4dd5-5cb3-4b27-8fe8-db5cc62c7316 this
]

```

- 세 번째 인스턴스 추가 시 멤버 리스트 업데이트

```jsx
Instance 1:
Members {size:3, ver:3} [
    Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268 this
    Member [172.19.248.90]:5702 - ea9e4dd5-5cb3-4b27-8fe8-db5cc62c7316
    Member [172.19.248.91]:5703 - 1623eadf-9cfb-4b83-9983-d80520cef753
]

Instance 2:
Members {size:3, ver:3} [
    Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268
    Member [172.19.248.90]:5702 - ea9e4dd5-5cb3-4b27-8fe8-db5cc62c7316 this
    Member [172.19.248.91]:5703 - 1623eadf-9cfb-4b83-9983-d80520cef753
]

Instance 3:
Members {size:3, ver:3} [
    Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268
    Member [172.19.248.90]:5702 - ea9e4dd5-5cb3-4b27-8fe8-db5cc62c7316
    Member [172.19.248.91]:5703 - 1623eadf-9cfb-4b83-9983-d80520cef753 this
]

```

- 데이터 업데이트 및 동기화
    - 세 인스턴스 모두 서로의 존재를 알고 있다.
    - 예를 들어, 인스턴스 1이 `cache.put()`을 통해 고객 프로필을 업데이트하면, 이 변경 사항은 모든 처리 장치에 비동기적으로 동기화되어 캐시가 항상 일관되게 유지

- 처리 장치 인스턴스 다운 시 업데이트
    - 처리 장치 인스턴스가 다운되며, 멤버 리스트가 자동으로 업데이트되고 다른 인스턴스는 이를 인식.
    - 예를 들어, 인스턴스 2가 다운되면, 인스턴스 1과 3의 멤버 리스트가 업데이트
    
    ```jsx
    Instance 1:
    Members {size:2, ver:4} [
        Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268 this
        Member [172.19.248.91]:5703 - 1623eadf-9cfb-4b83-9983-d80520cef753
    ]
    
    Instance 3:
    Members {size:2, ver:4} [
        Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268
        Member [172.19.248.91]:5703 - 1623eadf-9cfb-4b83-9983-d80520cef753 this
    ]
    
    ```
    

→ 이렇게 해서, **복제 캐시**는 각 처리 장치 간에 동기화되고, 처리 장치가 추가되거나 다운될 때도 자동으로 업데이트됩니다.

3. **처리 그리드**
- 필수 컴포넌트는 아니지만, 여러 처리 장치가 단일 비즈니스 요청을 처리할 때 오케스트레이션을 담당.
- 다양한 종류의 처리 장치(예: 주문 처리와 결제 처리) 사이의 요청을 중재하고 조정.
    <img width="704" alt="스크린샷 2024-11-17 오후 7 55 23" src="https://github.com/user-attachments/assets/8195d621-0e65-4499-ae28-775dde191627">

    

4. **배포 관리자**
- 부하 조건에 따라 처리 장치 인스턴스를 동적으로 시작하고 종료하는 컴포넌트.
- 응답 시간과 유저 부하를 모니터링하여 부하가 증가하면 새로운 처리 장치를 기동하고, 감소하면 기존 처리 장치를 종료.
- 애플리케이션의 확장성(탄력성) 요구사항을 구현하는 데 필수적인 컴포넌트.

---

### 15.1.3 데이터 펌프

- 데이터를 다른 프로세서에 보내 데이터베이스를 업데이트하는 장치.
- 공간 기반 아키텍처에서 처리 장치가 직접 데이터베이스를 읽고 쓰지 않으므로 필수적인 컴포넌트.
- 비동기적으로 동작하며, 메모리 캐시와 데이터베이스의 최종 일관성을 유지.
- 처리 장치가 캐시를 업데이트하면, 데이터 펌프를 통해 데이터베이스도 업데이트되도록 보장.
- 보통 메시징 기법을 사용해 비동기 통신, 메시지 순서 보장(FIFO), 처리 장치와 데이터 라이터 분리 등을 지원.
- 도메인별 또는 캐시 종류별로 여러 개 사용되며, 계약 데이터(추가, 삭제, 수정)에 대한 액션을 포함.
- 데이터는 일반적으로 새 값만 포함, 예: 고객 프로필에서 전화번호 변경 시 고객 ID, 업데이트 액션, 새 전화번호만 전송.
    <img width="698" alt="스크린샷 2024-11-17 오후 7 55 49" src="https://github.com/user-attachments/assets/2ba44e63-cf55-463f-84a1-a115715b4f0f">
    

---

### 15.1.4 데이터 라이터

- 데이터 펌프에서 받은 메시지를 바탕으로 데이터베이스를 업데이트하는 컴포넌트.
- 서비스, 애플리케이션, 또는 데이터 허브로 구현 가능.
- 데이터 라이터의 범위는 데이터 펌프와 처리 장치의 범위에 따라 달라짐.
- 도메인 기반 데이터 라이터는 특정 도메인(예: 고객)에 대한 전체 업데이트를 처리하는 모든 데이터베이스 로직을 포함.
    <img width="663" alt="스크린샷 2024-11-17 오후 7 56 00" src="https://github.com/user-attachments/assets/1b4747ba-7804-4ce5-82f9-9c9a883e9695">
    
- 예: 고객 도메인에서는 하나의 데이터 라이터가 여러 데이터 펌프를 리스닝하고, 고객 관련 테이블 레코드를 업데이트.
- 또 다른 모델에서는 각 처리 장치가 전용 데이터 라이터를 소유하고, 해당 처리 장치의 데이터베이스 로직을 처리.
- 이 모델은 확장성과 민첩성을 높이지만, 데이터 라이터가 많아지는 단점도 있음.
    <img width="690" alt="스크린샷 2024-11-17 오후 7 56 12" src="https://github.com/user-attachments/assets/bb449de8-20f2-4b75-84a1-0f164248d32e">
    

---

### 15.1.5 데이터 리더

1. **데이터 리더 (Data Reader)**
    - 데이터베이스에서 데이터를 읽고, 리버스 데이터 펌프(reverse datapump)를 통해 처리 장치로 데이터를 전달하는 컴포넌트.
    - 공간 기반 아키텍처에서 데이터 리더는 다음과 같은 세 가지 경우에만 작동:
        1. **모든 처리 장치 인스턴스 실패 시**: 모든 처리 장치가 실패하고, 데이터가 데이터베이스에서 읽어와야 할 때.
        2. **모든 처리 장치 재배포 시**: 동일한 이름의 캐시 안에서 모든 처리 장치를 재배포할 때.
        3. **아카이브 데이터 조회 시**: 복제 캐시에서 데이터가 없을 때, 데이터베이스에서 아카이브 데이터를 조회할 때.
    - 시스템 장애나 재배포 시, 첫 번째로 살아난 인스턴스가 캐시의 락을 획득하여 임시 캐시 소유자가 되고, 데이터를 요청하는 큐에 메시지를 보냄.
    - 데이터 리더가 데이터베이스에서 데이터를 읽고, 그 데이터를 리버스 데이터 펌프를 통해 처리 장치로 전달.
    - 임시 캐시 소유자는 데이터를 받아 캐시를 로드하고, 모든 처리 장치가 동기화되면 처리를 개시.
        <img width="658" alt="스크린샷 2024-11-17 오후 7 56 23" src="https://github.com/user-attachments/assets/ff2c7a34-ba85-4c40-8db7-14630d908218">
        
2. **데이터 리더/라이터**: 두 컴포넌트는 도메인 기반으로 사용되며, 일반적으로 특정 처리 장치 클래스 전용이다.
3. **데이터 추상 레이어 vs 데이터 액세스 레이어**: 데이터 액세스 레이어는 처리 장치가 데이터베이스 테이블 구조와 밀접하게 연결되고, 데이터 추상 레이어는 처리 장치와 데이터베이스가 분리되어 변경에 영향을 받지 않는다.
4. **변경 처리**: 데이터 리더/라이터는 데이터베이스 변경을 버퍼링하여 캐시로 반영할 수 있어 시스템의 안정성과 확장성을 지원.

---

## 15.2 데이터 충돌

1. **데이터 충돌**: 동일한 캐시를 포함한 서비스 인스턴스에서 업데이트가 동시에 발생할 경우, 복제 레이턴시로 인해 데이터 충돌이 발생할 수 있다. 예를 들어, 두 서비스 인스턴스가 동일한 제품 재고를 업데이트하는 도중에 서로 다른 값을 덮어씌워 데이터가 일관되지 않게 된다.
2. **충돌률 계산**: 데이터 충돌률은 캐시 크기, 업데이트율, 복제 레이턴시, 인스턴스 수 등에 영향을 받습니다. 예를 들어, 업데이트율 20 업데이트/초, 캐시 크기 50,000 로우, 복제 레이턴시 100밀리초일 경우, 충돌률은 시간당 14.4회로 예측된다.
3. **복제 레이턴시**: 복제 레이턴시가 낮을수록 데이터 충돌이 적다. 예를 들어, 레이턴시를 100밀리초에서 1밀리초로 줄이면 충돌률은 시간당 0.1회로 감소.
4. **캐시 크기와 충돌률**: 캐시 크기가 작을수록 충돌률이 증가. 캐시 크기가 50,000에서 10,000 로우로 줄어들면 충돌률은 급증.
5. **충돌률 관리**: 충돌률을 최소화하려면 최대 업데이트율을 기준으로 최소, 정상, 최대 충돌률을 계산하여 관리해야 한다.

---

### **15.3** 클라우드 대 온프레미스 구현

1. **배포 환경 선택지**: 공간 기반 아키텍처는 클라우드와 온프레미스 환경에서 모두 배포할 수 있으며, 두 환경을 결합한 하이브리드 클라우드 배포도 가능. 이러한 하이브리드 환경은 물리적인 데이터베이스를 온프레미스에 두고, 처리 장치와 가상 미들웨어를 클라우드에서 운영하는 방식.
2. **하이브리드 클라우드의 장점**: 이 아키텍처는 비동기 데이터 펌프와 최종 일관성 모델 덕분에 클라우드 기반의 데이터 동기화가 매우 효율적. 트랜잭션은 동적으로 탄력적인 클라우드 환경에서 처리되며, 물리적인 데이터와 리포팅, 데이터 분석 등은 안전하게 온프레미스 환경에 보관할 수 있다.
<img width="749" alt="스크린샷 2024-11-17 오후 7 56 35" src="https://github.com/user-attachments/assets/b476a3fa-e1aa-4e86-b1b5-490dd2f8c92c">

---

## 15.4 복제 캐시 대 분산 캐시

1. **공간 기반 아키텍처의 캐시 기술**: 공간 기반 아키텍처는 데이터베이스에 직접 읽기/쓰기를 하지 않고 캐시 기술을 활용하여 애플리케이션 트랜잭션을 처리하므로 확장성, 탄력성, 성능이 뛰어난다. 복제 캐시와 분산 캐시가 주요 캐시 모델로 사용된다.
2. **복제 캐시**: 복제 캐시는 각 처리 장치가 동일한 캐시를 사용하며, 캐시 업데이트 시 다른 처리 장치들도 자동으로 새로운 데이터로 업데이트된다. 이는 빠른 속도와 높은 내고장성을 지원하며, 중앙 서버에서 캐시를 관리하지 않으므로 단일 장애점이 없다. 그러나 처리 장치 간 데이터 복제를 제어하려면 외부 컨트롤러가 필요한 경우도 있다.
    <img width="682" alt="스크린샷 2024-11-17 오후 7 56 45" src="https://github.com/user-attachments/assets/9fe21f62-2b10-46b0-82e1-62db92c7c659">
    

3. **분산 캐시**: 데이터량이 많거나 캐시 데이터가 자주 업데이트되는 상황에서는 분산 캐시를 사용할 수 있다. 분산 캐시에서는 중앙 캐시 서버가 데이터를 관리하며, 각 처리 장치는 전용 프로토콜을 통해 중앙 캐시 서버에서 데이터를 가져온다. 이는 높은 데이터 일관성을 보장하지만, 성능이 떨어지고 시스템 레이턴시가 증가하는 단점이 있다. 또한, 캐시 서버 다운 시 데이터 액세스가 불가능해져 시스템 중단이 발생할 수 있다.
    <img width="691" alt="스크린샷 2024-11-17 오후 7 56 54" src="https://github.com/user-attachments/assets/7fc6e55c-b9ec-4f5e-8ae0-f553143674d6">
    

**복제 캐시 vs 분산 캐시 선택 기준**:

1. **데이터 일관성**:
    - **분산 캐시**: 데이터가 한 곳에 모여 있어, 복제 캐시보다 **우수한 데이터 일관성**을 제공한다. 데이터 업데이트가 빈번하고 일관성이 중요한 경우에 적합하다.
    - **복제 캐시**: 여러 처리 장치에 캐시가 분산되어 있어 데이터 일관성보다는 **성능과 내고장성**을 우선시한다. 일관성보다는 성능이 중요할 때 유리하다.
2. **성능과 내고장성**:
    - **복제 캐시**: 처리 장치 간 데이터 복제가 빠르고, **내고장성이 높으며** 중앙 서버가 필요 없어 **성능이 뛰어나다**. 하지만 데이터 일관성은 상대적으로 약할 수 있다.
    - **분산 캐시**: 중앙 서버에서 데이터를 관리하므로 **성능**이 낮고, **시스템 레이턴시**가 증가하지만 **내고장성**이 중요할 때 적합하다.
3. **데이터 유형에 따른 선택**:
    - **일관성이 중요한 데이터** (예: 제품 재고)에는 **분산 캐시**가 적합하다.
    - **성능이 중요한 데이터** (예: 제품 코드, 제품 설명)에는 **복제 캐시**가 적합하다.

---

## 15.5 니어 캐시

**니어 캐시 (Near-Cache)**는 분산 캐시와 인메모리 데이터 그리드를 결합한 하이브리드 캐시 모델입니다. 이 모델에서:

1. **분산 캐시 (Back-end Cache)**: 전체 데이터를 관리하며, **풀 백킹 캐시**로 사용됩니다. 이는 중앙에서 데이터를 보유하고, 각 처리 장치에서 캐시된 데이터는 작은 서브셋만을 담고 있습니다.
2. **인메모리 데이터 그리드 (Front-end Cache)**: 각 처리 장치에 포함된 **프런트 캐시**는 풀 백킹 캐시보다 작은 데이터를 보유하며, 데이터 접근 성능을 향상시키기 위해 사용됩니다.

프런트 캐시는 최신 데이터나 자주 사용되는 데이터를 우선적으로 담는 방출 정책을 따릅니다. 주요 방출 정책은 다음과 같습니다:

- **MRU (Most Recently Used)**: 최근 사용된 데이터를 저장합니다.
- **MFU (Most Frequently Used)**: 가장 자주 사용된 데이터를 저장합니다.
- **랜덤 교체 (Random Replacement)**: 특정 규칙 없이 무작위로 데이터를 삭제하여 공간을 확보합니다. 이 방식은 데이터 접근 패턴을 명확히 알 수 없을 때 유용합니다.

이 모델은 **성능 최적화**와 **데이터 일관성**을 동시에 고려할 수 있는 장점이 있습니다.
<img width="702" alt="스크린샷 2024-11-17 오후 7 57 10" src="https://github.com/user-attachments/assets/6f316174-8f58-4535-aee7-936be3578f70">


---

## 15.6 구현 예시

**공간 기반 아키텍처**는 유저 수나 요청량이 갑자기 폭증하는 애플리케이션이나 10,000명 이상의 동시 유저를 처리해야 하는 애플리케이션에 적합하다. 

예를 들어:

1. **온라인 콘서트 티켓 판매 시스템**: 이 시스템은 갑작스러운 트래픽 폭증을 처리하기 위해 높은 확장성, 탄력성, 고성능을 요구한다.
2. **온라인 경매 시스템**: 실시간으로 빠르게 변동하는 데이터와 높은 동시성 요구 사항을 처리할 수 있는 성능을 필요로 한다.

이러한 시스템들은 **확장성**과 **탄력성**을 기반으로, 성능 저하 없이 고성능을 보장하는 아키텍처가 필요하다.

---

### 15.6.1

**콘서트 티켓 판매 시스템**은 평소에는 동시 유저 수가 적지만, 인기 있는 콘서트의 티켓 발매가 시작되면 동시 유저 수가 급증하는 특징이 있다.

예를 들어:

1. **좌석 수와 티켓 수량**은 고정되어 있으며, 티켓 발매가 시작되면 **수백에서 수천** 명의 유저가 동시에 접속해 티켓을 구매하려 한다.
2. **빠른 데이터 업데이트**: 좌석 가능 여부는 매우 빠르게 업데이트되어야 하며, 특히 좌석 지정제 콘서트에서는 더욱 신속한 업데이트가 필요하다.

**공간 기반 아키텍처**는 이런 문제를 해결하는 데 적합하다. 시스템이 중앙 데이터베이스에 동기적으로 접근하는 대신, **분산된 처리 장치**를 활용해 동시 요청을 처리하며, **고도의 탄력성**을 제공한다. 

예를 들어:

- **배포 관리자 (Deployment Manager)**는 트래픽 급증을 실시간으로 감지하여, 필요한 수의 처리 장치를 즉시 기동시킨다.
- **사전 준비**: 티켓 발매 전, 예상되는 부하를 감당할 수 있도록 미리 충분한 처리 장치를 가동해두고, 급증하는 트래픽에 대응한다.

이러한 특성 덕분에, 콘서트 티켓 판매 시스템은 **확장성**과 **성능**을 최적화하며, 수많은 동시 요청을 원활히 처리할 수 있다.

---

### 15.6.2 온라인 경매 시스템

**온라인 경매 시스템**은 **콘서트 티켓 판매 시스템**과 유사한 아키텍처 특성을 가지고 있으며, 다음과 같은 특징이 있다:

1. **고도의 성능과 탄력성**이 필요하며, 유저 수와 요청 부하가 예기치 않게 급증할 수 있습다. 경매가 시작되면 참가자 수와 입찰자가 몇 명일지 예측할 수 없기 때문에, 예기치 않은 트래픽 급증을 처리할 수 있는 시스템이 필수.
2. **부하 증가 시 자동 확장**: 경매가 진행되면 부하가 급증할 수 있기 때문에, **공간 기반 아키텍처**는 **동적 확장성**을 제공한다. 시스템은 부하가 증가하면 여러 개의 처리 장치를 즉시 기동하여 문제를 해결.
3. **입찰 데이터의 일관성 보장**: 각 경매에 대해 처리 장치를 할당하여 데이터의 일관성을 유지할 수 있으며, 경매 종료 후 불필요한 처리 장치는 제거할 수 있다.
4. **비동기 데이터 전송**: **데이터 펌프**는 비동기적으로 작동하므로, 입찰 데이터를 긴 레이턴시 없이 다른 처리 장치로 전달할 수 있다 (예: 입찰 이력, 분석, 감사). 이로 인해 전체 입찰 프로세스의 성능이 향상.

**공간 기반 아키텍처**는 경매 시스템의 **확장성**과 **성능**을 최적화하면서, **데이터 일관성**과 **빠른 입찰 처리**를 보장하는 데 적합한 솔루션이다.

---

## 15.7 아키텍처 특성 등급

### **주요 특성**

1. **탄력성, 확장성, 성능**:
    - 이 아키텍처는 **탄력성**, **확장성**, **성능**이 뛰어난 강점을 가집니다(모두 별점 5개).
    - **인메모리 데이터 캐시**를 활용하고, 제약조건이 있는 데이터베이스를 사용하지 않기 때문에 **고성능**을 달성할 수 있다. 이 덕분에 수백만 명의 동시 유저도 처리 가능하다.
2. **단순성 및 시험성**:
    - 그러나, **단순성**과 **시험성**에서는 트레이드오프가 존재합니다.
    - **구조의 복잡성** 때문에 이 아키텍처는 **시험성**에서 별점 1개를 받는다. 이는 **고도의 확장성**과 **탄력성**을 시뮬레이션하는 복잡도 때문에 발생한다.
    - **최고 부하** 상황에서 수십만 명의 동시 유저를 테스트하는 것은 매우 복잡하고, 비용이 많이 들며, 실제로 많은 대용량 테스트는 **프로덕션 환경에서** 이루어지기 때문에 **리스크**가 따른다.
3. **비용**:
    - **비용** 역시 중요한 고려 사항이다.
    - **클라우드**나 **온프레미스**에서 높은 확장성과 탄력성을 구현하려면 **상용 캐시 제품**을 사용해야 하고, 이로 인해 **라이선스 비용**과 **리소스 사용**이 증가하여 상대적으로 **비용이 많이 든다**.
4. **분할 유형**:
    - 공간 기반 아키텍처는 **도메인 분할**과 **기술 분할**로 나뉜다.
    - **도메인 분할**은 시스템이 **매우 탄력적이고 확장 가능한** 시스템 유형에 따라 조정되며, **기술 분할**은 **데이터 펌프**와 같은 기술 요소들이 트랜잭션 처리 문제를 분리하는 방식으로 나타난다.
5. **퀀텀 수**:
    - **퀀텀 수**는 **유저 인터페이스** 설계와 **처리 장치 간 통신**에 따라 달라진다.
    - **동기 통신**을 하는 처리 장치들이 동일한 아키텍처 퀀텀에 속하며, **데이터베이스**는 퀀텀 방정식에 포함되지 않는다.

### **결론**:

공간 기반 아키텍처는 **탄력성, 확장성, 성능**에서 강점을 가지며, 고성능을 요구하는 시스템에 적합하지만, **구조적 복잡성**과 **시험성**에 있어서 도전 과제가 있다. 또한, **비용**이 높은 편이므로 이를 고려한 설계가 필요하다.
