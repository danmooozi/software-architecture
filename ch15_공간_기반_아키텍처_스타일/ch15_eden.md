# 공간 기반 아키텍처 스타일

웹 기반 비즈니스 애플리케이션은 브라우저, 웹 서버, 애플리케이션 서버, 데이터베이스 서버 순서로 요청을 처리합니다.  
유저가 증가하면 웹 서버에서 병목 현상이 시작되어, 애플리케이션 서버와 데이터베이스 서버로 확산됩니다.  
병목을 줄이기 위해 웹 서버를 확장할 수 있지만, 애플리케이션 서버와 데이터베이스 서버 쪽으로 병목 지점이 밀릴 뿐 근본적인 해결은 되지 않습니다.  
결국 데이터베이스 서버까지 확장을 하게 되겠지만, 웹 기반 구조는 웹 서버 확장은 쉽고, 데이터베이스는 확장이 어려운 삼각형 토폴로지 형태를 가지기 때문에 한계가 있습니다.

<img src="./images/15-1.png" width="500" />

동시적인 유저 부하가 많은 대용량 애플리케이션에서는, **데이터베이스의 동시 트랜잭션 처리 능력이 최종 제약 조건**이 되는 경우가 많습니다.  
캐시 기술과 데이터베이스 확장 솔루션을 통해 문제를 완화할 수 있지만, **높은 부하 상황에서 애플리케이션을 확장하는 작업은 여전히 쉽지 않습니다**.

공간 기반 아키텍처 스타일은 높은 확장성, 탄력성, 동시성 문제를 해결하도록 설계된 아키텍처 스타일로, 동시 유저 수가 예측하기 어려울 정도로 가변적인 애플리케이션에 적합합니다.  
데이터베이스를 확장하거나 캐시 기술을 적용하는 대신, 아키텍처적인 접근으로 확장성 문제를 효과적으로 해결합니다.

## 15.1 토폴로지

공간 기반 아키텍처라는 명칭은 튜플 공간(tuple space)에서 유래되었습니다.  
튜플 공간은 공유 메모리를 통해 다중 병렬 프로세서 간 통신이 이루어지는 기술입니다.  
공간 기반 아키텍처에서는 중앙 데이터베이스를 사용하는 대신 **복제된 인메모리 데이터 그리드(in-memory data grid)**를 활용해 확장성, 탄력성, 성능을 높입니다.  
**애플리케이션 데이터는 메모리에 저장**되어 **병렬 처리 장치들이 데이터를 복제**합니다.  
처리 장치가 데이터를 업데이트 할 때에는 **퍼시스턴스 큐(persistent queue)에 메시지를 전송**하는 식으로 데이터베이스에 데이터를 비동기로 전송됩니다.  
**처리 장치는 유저 부하에 따라 동적으로 추가 및 삭제**가 가능하며, 중앙 데이터베이스가 애플리케이션의 표준 트랜잭션 처리에 관여하지 않아 데이터베이스 병목 없기 때문에, 높은 확장성을 보장할 수 있습니다.

공간 기반 아키텍처는 다음의 주요 컴포넌트로 구성됩니다:

- **처리 장치(processing unit)**: 애플리케이션 코드
- **가상 미들웨어(virtualized middleware)**: 처리 장치를 관리하고 조정
- **데이터 펌프(data pump)**: 데이터를 비동기 전송
- **데이터 라이터(data writer)**: 데이터를 업데이트
- **데이터 리더(data reader)**: 처리 장치 시작 시 데이터베이스에서 데이터를 읽어 전달

### 15.1.1 처리 장치

처리 장치는 애플리케이션 로직(일부 또는 전체)을 포함하며, 어플리케이션 종류에 따라 다른 내용은 담고 있습니다.  
작은 웹 애플리케이션은 단일 처리 장치에 배포될 수 있지만, 대규모 애플리케이션은 기능에 따라 여러 처리 장치에 나누어 배치될 수 있습니다.  
마이크로서비스처럼 단일 목적의 작은 서비스도 처리 장치에 포함될 수 있으며, 어플리케이션 로직 외에도 인메모리 데이터 그리드 및 복제 엔진도 함께 포함될 수 있습니다.

<img src="./images/15-3.png" width="500" />

### 15.1.2 가상 미들웨어

가상 미들웨어는 아키텍처 내부에서 데이터 동기화와 요청 처리를 제어하는 인프라 역할을 합니다.  
**메시징 그리드**, **데이터 그리드**, **처리 그리드**, **배포 관리자**와 같은 컴포넌트로 구성되며, 이들 컴포넌트는 직접 작성하거나 서드파티 제품으로 구매할 수 있습니다.

#### 메시징 그리드(messaging grid)

메시징 그리드는 **입력 요청과 세션 상태를 관리**하며, 가상 미들웨어에 들어온 **요청을 적절한 활성 처리 장치로 전달**합니다.  
라운드 로빈 같은 단순한 알고리즘부터 요청 처리 상태를 추적하는 복잡한 알고리즘까지 다양한 방식으로 구현할 수 있으며, 보통 HA 프록시나 Nginx 같은 부하 분산이 가능한 웹 서버로 구현됩니다.

<img src="./images/15-4.png" width="500" />

#### 데이터 그리드

데이터 그리드는 이 아키텍처에서 핵심적인 컴포넌트입니다.  
일반적으로 복제 캐시로서 처리 장치 내에 구현되지만,  
외부 컨트롤러가 필요한 복제 캐시 구현체나 분산 캐시를 사용하는 경우 가상 미들웨어와 처리 장치 모두에 위치할 수 있습니다.  
메시징 그리드는 모든 처리 장치에 요청을 전달할 수 있으므로, **각 처리 장치는 동일한 데이터를 인메모리 데이터 그리드에 유지**해야 합니다.  
데이터 동기화는 비동기 방식으로 100밀리초 미만의 속도로 신속하게 이루어집니다.

<img src="./images/15-5.png" width="500" />

데이터는 동일한 이름의 데이터 그리드를 포함한 처리 장치 간에 동기화됩니다.  
아래는 헤이즐캐스트를 사용해 고객 프로필 정보를 저장하는 처리 장치에 대해 내부 복제 데이터 그리드를 생성하는 자바 코드 예시입니다.

```java
HazelcastInstance hz = Hazelcast.newHazelcastInstance();
Map<String, CustomerProfile> profileCache = hz.getReplicatedMap("CustomerProfile");
```

이 코드는 모든 처리 장치에 포함되며, 어느 처리 장치에서든 “CustomerProfile” 캐시에 변경이 발생하면 동일한 캐시를 가진 다른 모든 처리 장치에 데이터가 복제됩니다.  
각 처리 장치는 필요한 만큼의 복제 캐시를 소유할 수 있습니다.

데이터는 처리 장치 내에서 복제되므로 데이터베이스 접근 없이도 서비스 인스턴스의 기동/중지가 가능합니다.  
단, 하나 이상의 이름이 지정된 복제 캐시 인스턴스가 필요하며, 처리 장치 인스턴스가 시작되면 캐시 프로바이더에 연결하여 캐시를 요청합니다.  
만약 다른 처리 장치에 연결되면 일단 그곳에서 캐시를 로드합니다.

각 처리 장치는 멤버 리스트를 통해 다른 처리 장치 인스턴스들을 인식하며, 이 리스트에는 **동일한 이름의 캐시를 사용하는 모든 처리 장치의 IP 주소 및 포트가 포함**됩니다.  
예를 들어, 고객 프로필 코드와 복제 캐시 데이터를 가진 처리 장치가 3개 존재한다면, 헤이즐캐스트는 다음과 같이 멤버 리스트를 저장하여, 각 처리 장치의 IP 주소 및 포트를 관리합니다.

```
Instance 1:
Members {size:3, ver:3} [
    Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268 this
    Member [172.19.248.90]:5702 - ea9e4dd5-5cb3-4b27-8fe8-db5cc62c7316
    Member [172.19.248.91]:5703 - 1623eadf-9cfb-4b83-9983-d80520cef753
]

Instance 2:
Members {size:3, ver:3} [
    Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268
    Member [172.19.248.90]:5702 - ea9e4dd5-5cb3-4b27-8fe8-db5cc62c7316 this
    Member [172.19.248.91]:5703 - 1623eadf-9cfb-4b83-9983-d80520cef753
]

Instance 3:
Members {size:3, ver:3} [
    Member [172.19.248.89]:5701 - 04a6f863-dfce-41e5-9d51-9f4e356ef268
    Member [172.19.248.90]:5702 - ea9e4dd5-5cb3-4b27-8fe8-db5cc62c7316
    Member [172.19.248.91]:5703 - 1623eadf-9cfb-4b83-9983-d80520cef753 this
]
```

여기서 인스턴스 1이 고객 프로필 정보의 업데이트 요청을 받아서 캐시를 업데이트하면, 데이터 그리드(헤이즐캐스트)는 **다른 복제 캐시도 똑같이 비동기 업데이트**하는 식으로 고객 프로필 캐시를 항상 똑같이 맞춥니다.

#### 처리 그리드

처리 그리드는 여러 처리 장치가 단일 비즈니스 요청을 처리할 때, 이를 오케스트레이트하는 역할을 합니다.  
또한, 서로 다른 종류의 처리 장치 간 조정이 필요한 요청이 있을 때, 이를 중재/조정하는 역할을 합니다.

<img src="./images/15-6.png" width="500" />

#### 배포 관리자

배포 관리자는 응답 시간과 유저 부하를 모니터링하여,부하에 따라 처리 장치 인스턴스를 동적으로 시작 및 종료하는 컴포넌트 입니다.  
확장성과 탄력성을 충족하는 데 필수적입니다.

### 15.1.3 데이터 펌프

데이터 펌프는 데이터를 다른 프로세서로 비동기로 전송하여 데이터베이스를 업데이트하는 장치입니다.  
캐시 업데이트 이후 데이터베이스도 최종적으로 일관성 있게 업데이트되도록 하여, 메모리 캐시와 데이터베이스 간 최종 일관성을 유지합니다.

데이터 펌프는 주로 비동기 통신을 통한 메시징 기법으로 구현됩니다.  
메시징은 전달 보장을 지원하고, FIFO 큐로 메시지 순서를 유지합니다.  
메시징을 이용하면 처리 장치와 데이터 라이터를 분리할 수 있어서, 데이터 라이터가 장애로 중단 되어도 처리 장치는 계속 무중단 처리가 가능합니다.

<img src="./images/15-7.png" width="500" />

데이터 펌프는 일반적으로 도메인 또는 서브도메인별로 여러 개 사용됩니다.  
캐시 종류별로(CustomerProfile, CustomerWishlist) 전용 데이터 펌프를 두거나, 더일반적인 캐시를 포함한 큰 도메인별로(Customer) 배정할 수도 있습니다.

데이터 펌프는 계약 데이터와 관련된 추가, 삭제, 수정 액션을 포함하며, 계약 포맷은 JSON, XML 스키마, 객체, 값 기반 메시지 등 다양한 형식을 가집니다.  
데이터 업데이트 시, 보통 데이터 펌프에는 변경된 데이터 값만 저장합니다.  
예를 들어 고객 프로필에서 전화번호가 변경되면, 고객 ID, 업데이트 액션, 새 전화번호만 데이터 파이프에 전송됩니다.

### 15.1.4 데이터 라이터

데이터 라이터는 데이터 펌프에서 받은 메시지를 바탕으로 데이터베이스를 업데이트하는 컴포넌트입니다.  
도메인 기반 데이터 라이터는 특정 도메인의 업데이트를 처리하는데 필요한 모든 데이터베이스 로직을 가지고 있습니다.  
하나의 데이터 라이터가 여러 데이터 펌프를 리스닝하의, 해당 도메인의 모든 데이터 업데이트를 담당합니다.  
데이터 펌프는 어플리케이션으로 구현될 수도 있고, 데이터 허브로 구현될 수도 있습니다.

<img src="./images/15-8.png" width="500" />

처리 장치 클래스마다 전용 데이터 라이터를 두는 것도 가능합니다.  
이 경우, 각 데이터 라이터가 자신의 데이터 펌프와 데이터베이스 로직을 관리합니다.  
이렇게 하면 데이터 라이터 수가 많아지긴 하지만, 데이터 펌프와 데이터 라이터가 처리 장치마다 할당되어 확장성과 민첩성을 향상시킬 수 있습니다.

<img src="./images/15-9.png" width="500" />

### 15.1.5 데이터 리더

데이터 리더는 데이터베이스에서 데이터를 읽어, 리버스 데이터 펌프를 통해 처리 장치로 전달하는 컴포넌트입니다.  
데이터 리더는 다음의 세 가지 경우에 동작합니다.

1. 동일한 이름의 캐시를 가진 모든 처리 장치 인스턴스가 중단되었을 때
2. 동일한 이름의 캐시를 가진 모든 처리 장치를 재배포할 때
3. 복제 캐시에 없는 아카이브 데이터를 조회할 때

모든 처리 장치 인스턴스가 다운되면, 데이터베이스에서 데이터를 직접 읽어와야 합니다.  
이 때 처리 장치 인스턴스가 복구되기 시작하면, **복구된 인스턴스는 캐시에 락을 요청**하며, 첫 번째로 락을 획득한 인스턴스가 임시 캐시 소유자가 되고 나머지 인스턴스는 락이 해제될 때까지 기다리게 됩니다.  
**임시 캐시 소유자는 데이터를 요청하는 큐에 메시지**를 보내면, **데이터 리더는 데이터베이스에서 필요한 데이터를 검색해 리버스 데이터 펌프를 통해 전달**합니다.  
임시 캐시 소유자는 데이터를 캐시에 로드한 후 락을 해제하고, 모든 인스턴스가 동기화되면 처리를 시작합니다.

<img src="./images/15-10.png" width="500" />

데이터 라이터와 데이터 리더는 데이터 추상 레이어(data abstraction layer) 또는 데이터 액세스 레이어(data access layer)를 형성합니다.  
**데이터 액세스 레이어**는 처리 장치가 데이터 하부 구조에 밀접하게 연계된 구조로, 데이터 리더/라이터를 통해 데이터베이스에 접근하여 **내부 로직은 데이터에 커플링** 되어 있습니다.  
반면, **데이터 추상 레이어**는 데이터 리더/라이터와 별도의 계약을 맺어서, 처리 장치가 하부 데이터 구조와 분리되도록 합니다.  
이를 통해 데이터베이스의 변경(컬럼/테이블 삭제)이 처리 장치에 영향을 주지 않게 됩니다.  
공간 기반 아키텍처는 주로 데이터 추상 레이어 모델을 사용하여, 처리 장치의 복제 캐시 스키마와 데이터베이스 테이블 구조가 별도로 정의될 수 있습니다.  
데이터 리더/라이터에 둘 사이의 변환 로직을 위치시켜서, 데이터 구조가 변경되도 처리 장치에 영향이 없게 합니다.

## 15.2 데이터 충돌

active/active 상태에서 복제 캐시를 사용할 경우, 복제 지연으로 인해 데이터 충돌이 발생할 수 있습니다.  
데이터 충돌은 한 캐시 인스턴스(A)에서 데이터가 업데이트되고 이를 다른 인스턴스(B)에 복제하는 동안, 해당 캐시(B)에서도 동일한 데이터가 업데이트되는 상황을 의미합니다.  
결과적으로 캐시 A와 B에서 서로의 업데이트가 덮어씌워져 데이터 불일치가 발생합니다.  
예를 들어 제품 재고 관리 서비스에서 다음과 같이 데이터 충돌이 발생할 수 있습니다.

1. 파란색 제품의 현재 재고는 500개다.
2. 서비스 A는 파란색 제품의 재고 캐시를 490개(10개 판매)로 업데이트한다.
3. 캐시를 복제하는 동안 서비스 B는 파란색 제품의 재고 캐시를 495개(5개 판매)로 업데이트한다.
4. 서비스 A의 업데이트가 복제되어 서비스 B의 재고 캐시는 490개로 업데이트된다.
5. 서비스 B의 업데이트가 복제되어 서비스 A의 재고 캐시는 495개로 업데이트된다.
6. 결과적으로 서비스 A와 B의 캐시 값이 일치하지 않으며, 실제 재고는 485개여야 하지만 동기화가 어긋난다.

데이터 충돌은 여러 팩터가 영향을 미치며, 데이터 충돌률 (CR)을 다음과 같은 수식으로 계산할 수 있습니다:

`CR = N * (UR^2 / S) * RL`

- **N** : 동일한 이름의 캐시를 사용하는 서비스 인스턴스 수
- **UR** : 밀리초 당 업데이트율
- **S** : 캐시 크기 (로우 개수)
- **RL** : 캐시 제품의 복제 대기 시간

이 공식은 예상 데이터 충돌률(%)을 계산해, 복제 캐시가 쓸만 한지를 평가하는 데 유용합니다.  
예를 들어, 다음 값을 대입하면:

다음 값을 대입하면:

- 업데이트율(UR): 20 업데이트/초
- 인스턴스 수(N): 5
- 캐시 크기(S): 50,000 로우
- 복제 레이턴시(RL): 100 밀리초
- 업데이트: 시간 당 72,000
- 충돌률: 시간 당 14.4
- 비율: 0.02%

계산 결과 시간당 72,000개의 업데이트가 발생하며, 이 중 약 14건의 데이터 충돌이 예상됩니다.  
충돌률 0.02%로 낮은 편이므로, 복제 캐시를 사용해도 어느 정도 괜찮음을 알 수 있습니다.

가변적인 복제 레이턴시는 데이터 일관성에 중요한 영향을 미치며, 다양한 팩터에 영향을 받기 때문에 정확한 값을 알기 위해서는 프로덕션 환경에서 직접 측정해야 합니다.  
복제 레이턴시와 충돌률은 서로 비례하며, 복제 레이턴시를 100밀리초에서 1밀리초로 변경한 경우, 충돌률은 시간당 0.1로 매우 낮아집니다.

- **업데이트율 (UR)** : 20 업데이트/초
- **인스턴스 수 (N)** : 5
- **캐시 크기 (S)** : 50,000 로우
- **복제 레이턴시 (RL)** : 1 밀리초 (100밀리초에서 변경됨)
- **업데이트** : 시간 당 72,000
- **충돌률** : 시간 당 0.1
- **백분율** : 0.0002%

처리 인스턴스 장치 수는 데이터 충돌률과 비례합니다.  
예를 들어, 처리 인스턴스 장치 수를 5개에서 2개로 줄인 경우 충돌률은 5.8개로 낮아집니다.:

- **업데이트율 (UR)** : 20 업데이트/초
- **인스턴스 수 (N)** : 2 (5에서 변경됨)
- **캐시 크기 (S)** : 50,000 로우
- **복제 레이턴시 (RL)** : 100 밀리초
- **업데이트** : 시간 당 72,000
- **충돌률** : 시간 당 5.8
- **백분율** : 0.008%

캐시 크기는 데이터 충돌률과 반비례 관계에 있으므로,
캐시 크기를 줄이면 충돌률이 증가합니다.  
예를 들어, 캐시 크기를 50,000에서 10,000로 줄인 경우 충돌률은 72로 증가합니다.:

- **업데이트율 (UR)** : 20 업데이트/초
- **인스턴스 수 (N)** : 2
- **캐시 크기 (S)** : 10,000 로우 (50,000에서 변경됨)
- **복제 레이턴시 (RL)** : 100 밀리초
- **업데이트** : 시간 당 72,000
- **충돌률** : 시간 당 72.0
- **백분율** : 0.1%

대부분의 시스템은 지속적으로 높은 업데이트율을 유지하지 않으므로, 충돌률 계산 시에는 최대 업데이트율을 기준으로 산출하는 것이 좋습니다.

## 15.3 클라우드 vs 온프레미스 구현

공간 기반 아키텍처는 배포 환경에서 독특한 선택지를 제공합니다.  
전체 토폴로지는 다음 환경에 배포할 수 있습니다:

- **클라우드 기반 환경** : 모든 컴포넌트를 클라우드에 배포
- **온프레미스 환경** : 모든 컴포넌트를 온프레미스 환경에 배포
- **하이브리드 클라우드 환경** : 물리 데이터베이스와 데이터를 온프레미스에 두고, 클라우드의 관리형 환경에 처리 장치와 가상 미들웨어를 배포

공간 기반 아키텍처는 하이브리드 클라우드 환경을 통해 클라우드와 온프레미스의 장점을 적절히 배합할 수 있다는 장점을 가집니다.  
트랜잭션은 탄력적인 클라우드 기반의 환경에서 처리하되, 물리적인 데이터는 안전한 온프레미스 환경에 보관할 수 있습니다.

## 15.4 복제 캐시 vs 분산 캐시

공간 기반 아키텍처에서는 캐시 기술을 통해 애플리케이션 트랜잭션을 처리하여, 데이터베이스에 직접 접근하지 않아도 되기 때문에, 확장성, 탄력성, 성능이 우수합니다.  
이 때 복제 캐시 또는 분산 캐시를 사용할 수 있습니다. (일반적으로은 복제 캐시 활용)

복제 캐시 사용 시, 이름이 동일한 캐시를 사용하는 처리 장치들은 동기화된 인메모리 데이터 그리드를 가지고 있습니다.  
하나의 처리 장치에서 캐시가 업데이트되면 다른 처리 장치들도 자동으로 업데이트됩니다.
복제 캐시는 속도가 매우 빠르고, 높은 내고장성을 제공하며, 중앙 서버에 의존하지 않아 단일 장애점이 없다는 장점이 있습니다.

하지만 복제 캐시는 데이터량이 많거나 업데이트가 빈번한 경우 사용이 어렵습니다.  
특히 캐시가 100MB를 초과하면 각 처리 장치의 메모리를 많이 점유하게 되어, 확장성과 탄력성이 저하됩니다.  
일반적으로 처리 장치는 VM에 배포되는데, 각 VM마다 할당된 메모리만큼만 내부 캐시로 쓸 수 있으므로, 처리량에 압박이 있는 상황에서는 처리 장치 인스턴스 수를 제한하게 됩니다.  
또한, 캐시 업데이트 속도가 매우 높을 때에는 데이터 그리드의 동기화 작업의 속도가 이에 미치지 못해, 데이터 충돌이 발생할 수 있습니다.

이런 상황에서는 분산 캐시를 사용할 수 있습니다.  
분산 캐시 사용 시에는 외부에 중앙 캐시 서버를 두고, 처리 장치들에서 해당 서버의 데이터에 엑세스하도록 하여 높은 데이터 일관성을 보장합니다.  
하지만 원격에서 데이터를 가져와야 하므로, 성능이 복제 캐시보다 낮고 레이턴시가 증가합니다.  
내고장성 측면에서도 중앙 캐시 서버가 다운되면 시스템이 중단될 수 있다는 문제가 있습니다.  
분산 캐시를 미러링하여 내고장성을 일부 개선할 수 있지만, 메인 서버의 예기치 못한 다운으로 인해 데이터가 제대로 동기화 되지 못했다면 데이터 일관성 문제가 발생할 수 있습니다.

캐시 크기가 작고 업데이트 속도가 느리다면, 복제 캐시와 분산 캐시 중 무엇을 선택할지는 **데이터 일관성과 성능/내고장성의 우선순위에 따라 결정**됩니다.  
분산 캐시는 중앙 집중형으로 데이터 일관성이 뛰어나며, 복제 캐시는 성능과 내고장성이 우수합니다.  
따라서 **일관성이 중요한 데이터(예: 제품 재고)에는 분산 캐시** 를, **자주 변경되지 않는 데이터(예: 이름/값 쌍, 제품 코드, 제품 설명)에는 복제 캐시** 를 사용하는 것이 적절합니다.

| 결정 기준     | 복제 캐시     | 분산 캐시         |
| ------------- | ------------- | ----------------- |
| 최적화        | 성능          | 일관성            |
| 캐시 크기     | 작다 (<100MB) | 크다 (500MB 이상) |
| 데이터 유형   | 비교적 정적임 | 매우 동적임       |
| 업데이트 빈도 | 비교적 낮음   | 매우 높음         |
| 내고장성      | 좋음          | 나쁨              |

## 15.5 니어 캐시(near-cache)

**니어 캐시는 분산 캐시와 인메모리 데이터 그리드를 결합한 하이브리드 캐시 모델**입니다.  
이 모델에서 분산 캐시는 full backing cache, 각 처리 장치에 있는 인메모리 데이터 그리드는 front cache 역할을 합니다.  
front cache는 full cache의 일부만 저장하며, 방출 정책(eviction policy)을 통해 오래된 항목을 제거하고 최신 항목을 추가합니다.  
주로 MRU(Most Recently Used, 최신 사용) 또는 MFU(Most Frequently Used, 빈번 사용) 캐시 방식을 사용하며, 최신 사용/빈번 사용 데이터를 식별하기 어려운 경우에는 RR(Random Replacement, 랜덤 교체) 방식을 적용합니다.

front cache는 full backing cache와 동기화되고, 각 처리 장치의 front cache끼리는 동기화되지 않습니다.  
따라서 각 처리 장치에서 동일한 컨텍스트 데이터를 다른 값으로 보유할 수 있습니다.(ex. 처리 장치마다 한 고객의 프로필을 다르게 가지고 있음)  
이로 인해 처리 장치들의 성능과 응답성이 일관되지 않게 나타나기 때문에, 니어 캐시 모델은 권장되지 않습니다.

## 15.6 구현 예시

공간 기반 아키텍처는 유저 수나 요청량이 급증하는 애플리케이션, 특히 10,000명 이상의 동시 유저를 처리해야 하는 애플리케이션에 적합합니다.  
온라인 콘서트 티켓 판매 시스템과 온라인 경매 시스템이 좋은 예로, 이들은 높은 확장성, 탄력성, 고성능이 요구됩니다.

### 15.6.1 콘서트 티켓 판매 시스템

콘서트 티켓 판매 시스템은 평소에는 동시 유저 수가 적지만, 인기 콘서트 티켓 발매 시 수백에서 수만까지 유저 수가 급증하는 패턴을 보입니다.  
유명 아티스트 티켓은 수 분 내 매진되므로 공간 기반 아키텍처의 확장성과 고성능이 필요합니다.  
티켓 수량은 한정되어 있기 때문에, 엄청난 동시 요청을 처리하면서 좌석 선택 가능 여부가 빠르게 업데이트되어야 합니다.  
이 때 일반적인 데이터베이스는 수만 개의 동시 트랜잭션을 처리할 수 없기 때문에, 매 요청마다 중앙 데이터베이스에 접근하면 시스템이 멈출 위험이 있습니다.

공간 기반 아키텍처는 고도의 탄력성을 제공하기 때문에, 티켓 판매 시스템에 적용하기 적합합니다.  
동시 유저 수가 급증하면, 배포 관리자가 이를 감지해 추가 처리 장치를 기동하여 대응합니다.  
가장 이상적인 방법은 티켓 발매 전에 처리 장치를 미리 준비해서, 유저 부하가 급증하기 직전까지 유휴 상태로 대기시키는 것입니다.

### 15.6.2 온라인 경매 시스템

온라인 경매 시스템은 경매 시작 시 참여 인원과 동시 입찰자 수를 예측하기 어렵다는 특징을 가집니다.  
유저와 요청 부하의 예기치 않은 폭증에 대처하기 위해 고성능과 탄력성이 요구됩니다.

공간 기반 아키텍처는 부하가 증가할 때 여러 처리 장치를 기동하는 식으로 대응이 가능하고, 경매가 끝나면 사용하지 않는 처리 장치를 쉽게 제거할 수 있습니다.  
각 경매마다 처리 장치를 할당하는 식으로 입찰 데이터의 일관성을 유지할 수도 있습니다.  
또한, 데이터 펌프를 통해 입찰 데이터를 레이턴시 없이 비동기로 다른 처리 장치(입찰 이력, 입찰 분석, 감사)에 전송할 수 있어, 입찰 프로세스의 성능을 개선할 수 있습니다.

## 15.7 아키텍처 특성 등급

| 아키텍처 특성 | 값                |
| ------------- | ----------------- |
| 분할 유형     | 도메인 + 기술     |
| 퀀텀 수       | 하나 또는 여러 개 |
| 배포성        | X X X             |
| 탄력성        | X X X X X         |
| 진화성        | X X X             |
| 내고장성      | X X X             |
| 모듈성        | X X X             |
| 전체 비용     | X X               |
| 성능          | X X X X X         |
| 신뢰성        | X X X X           |
| 확장성        | X X X X X         |
| 단순성        | X                 |
| 시험성        | X                 |

공간 기반 아키텍처는 탄력성, 확장성, 성능 면에서 최고 수준(별 5개)을 자랑합니다.  
인메모리 데이터 캐시를 활용하고 데이터베이스 제약을 제거했기 때문에, 수백만 명의 동시 유저도 원활히 처리할 수 있습니다.

하지만 공간 기반 아키텍처는 구조가 복잡하기 때문에, 단순성, 시험성, 비용 측면에서 트레이드오프가 있습니다.  
주요 데이터 저장소로 캐시를 사용하고, 최종 일관성을 실현하기 위해 복잡한 구조로 구성됩니다.  
일부 처리 장치에서 충돌이 발생해도 데이터 소실이 일어나지 않게 하기 위해 특별한 주의가 필요합니다.  
공간 기반 아키텍처는 시험성도 매우 낮습니다.  
수십만 명의 동시 유저가 요청하는 최고 부하 상태를 테스트하는 것은 비용이 많이 들며, 이로 인해 프로덕션 환경에서 바로 테스트가 수행되는 경우가 많아 리스크가 큽니다.  
또한 높은 확장성과 탄력성을 구현하기 위해서는 상용 캐시 제품을 사용해야 하므로 라이선스 비용을 지불해야 하고, 리소스 사용률도 높은 편이어서 비용이 많이 듭니다.

공간 기반 아키텍처는 도메인 분할과 기술 분할의 특징을 함께 포함됩니다.  
아키텍처는 탄력적이고 확장 가능한 시스템을 위해 도메인에 맞게 조정되며, 처리 장치도 유연하게 도메인으로 분할되어 도메인 서비스로 작동할 수 있습니다.  
이와 동시에, 트랜잭션 문제를 해결하기 위해 기술적 분할을 통해 데이터 펌프를 이용하여 캐시와 데이터베이스를 분할하고 있습니다.  
처리 장치, 데이터 펌프, 데이터 리더/라이터, 데이터베이스가 요청을 처리하는 기술 레이어를 구성하고 있는데, 이는 모놀리식의 n-티어 구조와 유사합니다.

공간 기반 아키텍처의 퀀텀 수는 유저 인터페이스 설계와 처리 장치 간 통신 방식에 따라 달라집니다.  
처리 장치가 데이터베이스와 동기 통신하지 않으므로 데이터베이스 자체는 퀀텀 방정식에 포함되지 않으며, 다양한 유저 인터페이스와 처리 장치가 연관된 형태로 퀀텀이 나타납니다.  
상호 동기 통신을 하거나, 요청을 오케스트레이션 하기 위해 처리 그리드를 거쳐 동기 통신을 하는 처리 장치는 동일한 아키텍처 퀀텀에 속하게 됩니다.
